	.version 1.4
	.target sm_10, map_f64_to_f32
	// compiled with /usr/local/cuda/open64/lib//be
	// nvopencc 4.0 built on 2011-03-20

	//-----------------------------------------------------------
	// Compiling /tmp/tmpxft_00001153_00000000-9_Spoc_kernels.cpp3.i (/tmp/ccBI#.7No3L4)
	//-----------------------------------------------------------

	//-----------------------------------------------------------
	// Options:
	//-----------------------------------------------------------
	//  Target:ptx, ISA:sm_10, Endian:little, Pointer Size:64
	//  -O3	(Optimization level)
	//  -g0	(Debug level)
	//  -m2	(Report advisories)
	//-----------------------------------------------------------

	.file	1	"<command-line>"
	.file	2	"/tmp/tmpxft_00001153_00000000-8_Spoc_kernels.cudafe2.gpu"
	.file	3	"/usr/lib/gcc/x86_64-linux-gnu/4.4.5/include/stddef.h"
	.file	4	"/usr/local/cuda/include/crt/device_runtime.h"
	.file	5	"/usr/local/cuda/include/host_defines.h"
	.file	6	"/usr/local/cuda/include/builtin_types.h"
	.file	7	"/usr/local/cuda/include/device_types.h"
	.file	8	"/usr/local/cuda/include/driver_types.h"
	.file	9	"/usr/local/cuda/include/surface_types.h"
	.file	10	"/usr/local/cuda/include/texture_types.h"
	.file	11	"/usr/local/cuda/include/vector_types.h"
	.file	12	"/usr/local/cuda/include/device_launch_parameters.h"
	.file	13	"/usr/local/cuda/include/crt/storage_class.h"
	.file	14	"/usr/include/bits/types.h"
	.file	15	"/usr/include/time.h"
	.file	16	"Spoc_kernels.cu"
	.file	17	"/usr/local/cuda/include/common_functions.h"
	.file	18	"/usr/local/cuda/include/math_functions.h"
	.file	19	"/usr/local/cuda/include/math_constants.h"
	.file	20	"/usr/local/cuda/include/device_functions.h"
	.file	21	"/usr/local/cuda/include/sm_11_atomic_functions.h"
	.file	22	"/usr/local/cuda/include/sm_12_atomic_functions.h"
	.file	23	"/usr/local/cuda/include/sm_13_double_functions.h"
	.file	24	"/usr/local/cuda/include/sm_20_atomic_functions.h"
	.file	25	"/usr/local/cuda/include/sm_20_intrinsics.h"
	.file	26	"/usr/local/cuda/include/surface_functions.h"
	.file	27	"/usr/local/cuda/include/texture_fetch_functions.h"
	.file	28	"/usr/local/cuda/include/math_functions_dbl_ptx1.h"


	.entry vec_add (
		.param .u64 __cudaparm_vec_add_A,
		.param .u64 __cudaparm_vec_add_B,
		.param .u64 __cudaparm_vec_add_C,
		.param .s32 __cudaparm_vec_add_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<10>;
	.reg .f32 %f<5>;
	.loc	16	7	0
$LDWbegin_vec_add:
	.loc	16	10	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 4;
	ld.param.u64 	%rd3, [__cudaparm_vec_add_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f32 	%f1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_add_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f32 	%f2, [%rd6+0];
	add.f32 	%f3, %f1, %f2;
	ld.param.u64 	%rd7, [__cudaparm_vec_add_C];
	add.u64 	%rd8, %rd7, %rd2;
	st.global.f32 	[%rd8+0], %f3;
	.loc	16	11	0
	exit;
$LDWend_vec_add:
	} // vec_add

	.entry vec_mult (
		.param .u64 __cudaparm_vec_mult_A,
		.param .u64 __cudaparm_vec_mult_B,
		.param .u64 __cudaparm_vec_mult_C,
		.param .s32 __cudaparm_vec_mult_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<10>;
	.reg .f32 %f<5>;
	.loc	16	14	0
$LDWbegin_vec_mult:
	.loc	16	17	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 4;
	ld.param.u64 	%rd3, [__cudaparm_vec_mult_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f32 	%f1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_mult_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f32 	%f2, [%rd6+0];
	mul.f32 	%f3, %f1, %f2;
	ld.param.u64 	%rd7, [__cudaparm_vec_mult_C];
	add.u64 	%rd8, %rd7, %rd2;
	st.global.f32 	[%rd8+0], %f3;
	.loc	16	18	0
	exit;
$LDWend_vec_mult:
	} // vec_mult

	.entry vec_div (
		.param .u64 __cudaparm_vec_div_A,
		.param .u64 __cudaparm_vec_div_B,
		.param .u64 __cudaparm_vec_div_C,
		.param .s32 __cudaparm_vec_div_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<10>;
	.reg .f32 %f<5>;
	.loc	16	20	0
$LDWbegin_vec_div:
	.loc	16	23	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 4;
	ld.param.u64 	%rd3, [__cudaparm_vec_div_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f32 	%f1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_div_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f32 	%f2, [%rd6+0];
	div.full.f32 	%f3, %f1, %f2;
	ld.param.u64 	%rd7, [__cudaparm_vec_div_C];
	add.u64 	%rd8, %rd7, %rd2;
	st.global.f32 	[%rd8+0], %f3;
	.loc	16	24	0
	exit;
$LDWend_vec_div:
	} // vec_div

	.entry vec_sub (
		.param .u64 __cudaparm_vec_sub_A,
		.param .u64 __cudaparm_vec_sub_B,
		.param .u64 __cudaparm_vec_sub_C,
		.param .s32 __cudaparm_vec_sub_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<10>;
	.reg .f32 %f<5>;
	.loc	16	26	0
$LDWbegin_vec_sub:
	.loc	16	29	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 4;
	ld.param.u64 	%rd3, [__cudaparm_vec_sub_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f32 	%f1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_sub_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f32 	%f2, [%rd6+0];
	sub.f32 	%f3, %f1, %f2;
	ld.param.u64 	%rd7, [__cudaparm_vec_sub_C];
	add.u64 	%rd8, %rd7, %rd2;
	st.global.f32 	[%rd8+0], %f3;
	.loc	16	30	0
	exit;
$LDWend_vec_sub:
	} // vec_sub

	.entry vec_fma (
		.param .u64 __cudaparm_vec_fma_A,
		.param .u64 __cudaparm_vec_fma_B,
		.param .u64 __cudaparm_vec_fma_C,
		.param .u64 __cudaparm_vec_fma_D,
		.param .s32 __cudaparm_vec_fma_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<12>;
	.reg .f32 %f<6>;
	.loc	16	32	0
$LDWbegin_vec_fma:
	.loc	16	35	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 4;
	ld.param.u64 	%rd3, [__cudaparm_vec_fma_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f32 	%f1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_fma_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f32 	%f2, [%rd6+0];
	ld.param.u64 	%rd7, [__cudaparm_vec_fma_C];
	add.u64 	%rd8, %rd7, %rd2;
	ld.global.f32 	%f3, [%rd8+0];
	mad.f32 	%f4, %f2, %f3, %f1;
	ld.param.u64 	%rd9, [__cudaparm_vec_fma_D];
	add.u64 	%rd10, %rd9, %rd2;
	st.global.f32 	[%rd10+0], %f4;
	.loc	16	36	0
	exit;
$LDWend_vec_fma:
	} // vec_fma

	.entry vec_add_64 (
		.param .u64 __cudaparm_vec_add_64_A,
		.param .u64 __cudaparm_vec_add_64_B,
		.param .u64 __cudaparm_vec_add_64_C,
		.param .s32 __cudaparm_vec_add_64_N)
	{
	.reg .u16 %rh<4>;
	.reg .u32 %r<5>;
	.reg .u64 %rd<10>;
	.reg .f64 %fd<5>;
	.loc	16	39	0
$LDWbegin_vec_add_64:
	.loc	16	42	0
	cvt.u32.u16 	%r1, %tid.x;
	mov.u16 	%rh1, %ctaid.x;
	mov.u16 	%rh2, %ntid.x;
	mul.wide.u16 	%r2, %rh1, %rh2;
	add.u32 	%r3, %r1, %r2;
	cvt.s64.s32 	%rd1, %r3;
	mul.wide.s32 	%rd2, %r3, 8;
	ld.param.u64 	%rd3, [__cudaparm_vec_add_64_A];
	add.u64 	%rd4, %rd3, %rd2;
	ld.global.f64 	%fd1, [%rd4+0];
	ld.param.u64 	%rd5, [__cudaparm_vec_add_64_B];
	add.u64 	%rd6, %rd5, %rd2;
	ld.global.f64 	%fd2, [%rd6+0];
	add.f64 	%fd3, %fd1, %fd2;
	ld.param.u64 	%rd7, [__cudaparm_vec_add_64_C];
	add.u64 	%rd8, %rd7, %rd2;
	st.global.f64 	[%rd8+0], %fd3;
	.loc	16	43	0
	exit;
$LDWend_vec_add_64:
	} // vec_add_64

